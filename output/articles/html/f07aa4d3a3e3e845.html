<!DOCTYPE html>
<html>
<head>
    <title>Opinion | Why People Can’t Quit ChatGPT - The New York Times</title>
    <meta charset="utf-8">
</head>
<body>
    <h1>Opinion | Why People Can’t Quit ChatGPT - The New York Times</h1>
    
    <div class="instapaper-info">
        <p><strong>Source:</strong> Additional Content (Archive Folder)</p>
        <p><strong>Content Length:</strong> 10,760 characters</p>
        <p><strong>Bookmark ID:</strong> 1858976170</p>
        <p><strong>Extraction:</strong> Maximization Discovery</p>
    </div>
    
    <div class="content">
        <pre><article><header><div><p>Guest Essay</p></div><div><time>July 16, 2025</time></div><div><figure><div><picture><img src="https://static01.nyt.com/images/2025/07/17/opinion/11weatherby/11weatherby-jumbo.jpg?quality=75&amp;auto=webp"></picture></div><figcaption><span><span>Credit...</span><span><span>Illustration by Ben Denzer; source photograph by Paul Denzer.</span></span></span></figcaption></figure></div><div><div><div><div><div><div>Listen to this article · 6:38 min <span><a href="https://help.nytimes.com/hc/en-us/articles/24318293692180">Learn more</a></span></div></div></div></div></div></div><div><div><div><p><span>By </span></p><div><div><p>Dr. Weatherby is the director of the Digital Theory Lab at New York University.</p></div></div></div></div></div></header><section><div><div><p>This spring, OpenAI’s C.E.O., Sam Altman, <a href="https://x.com/sama/status/1899535387435086115?lang=en">advertised</a> a new model of ChatGPT by showcasing its ability to write fiction. He had prompted the bot to write a story about grief, in the style of metafiction (a self-reflexive genre in which the narrator weaves personal details into the story). It duly generated a winding tale that compares grieving a dead loved one to the loss function, technical jargon for a bit of the math that makes modern artificial intelligence systems work. Mr. Altman crowed about the passage, implying that such a complex genre, one associated with pretentious literary types, could be written only by a really intelligent agent.</p><p>I’m a professor of literature, and I think the story is a solid illustration of the genre. I don’t know that it’s great literature or that ChatGPT is about to take over literary publishing. I certainly don’t think it proves that ChatGPT is intelligent; it just shows that it is an expert imitator of style.</p><p>More broadly, I think we’re having the wrong debates about A.I. altogether. In a <a href="https://www.theatlantic.com/culture/archive/2025/06/artificial-intelligence-illiteracy/683021/">recent article</a> for The Atlantic, Tyler Austin Harper called A.I. a “scam,” an animatronic simulation of intelligence. This claim went against not just Mr. Altman, but also many <a href="https://www.nytimes.com/2025/03/14/technology/why-im-feeling-the-agi.html">tech journalists</a> and <a href="https://www.natesilver.net/p/its-time-to-come-to-grips-with-ai">data wonks</a> who think Silicon Valley’s narrative that we are close to real machine intelligence is plausible. The linguist Emily Bender and the sociologist Alex Hanna think that <a href="https://urldefense.proofpoint.com/v2/url?u=https-3A__8cmbykf0.r.us-2Deast-2D1.awstrack.me_L0_https-3A-252F-252Fthecon.ai-252F_1_01000197d17d1a16-2Da6af5820-2D0f83-2D4268-2D8ff5-2Dc3cbcd2e6dce-2D000000_SBn1caYswVwkcfn4wDHizaKHtHo-3D433&amp;d=DwMFaQ&amp;c=slrrB7dE8n7gBJbeO0g-IQ&amp;r=bMQrB1JqPGH3QuzFgKxGwWv8_wlytMz_q_xKdjxXCE0&amp;m=eToEtV5cJJMiE_0HECHfwEt1IUWx1Aw5CAIusRC1sry-Hi-QMe1_5NruuDMpl8n5&amp;s=lluxMd5lYlFASkcnO8uBINauUMUVhA85YNMSPJXbSHg&amp;e=">A.I. is a con</a>, and Dr. Bender describes it as a set of tricks that produces “synthetic text” rather than human meaning.</p><p>These criticisms do little to explain A.I.’s popularity. They miss the fact that humans love to play games with language, not just use it to test intelligence. What Mr. Altman inadvertently showed us is that what is really driving the hype and widespread use of large language models like ChatGPT is that they are fun. A.I. is a form of entertainment.</p></div></div><div><div><p>OpenAI seems to understand this. ChatGPT was <a href="https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/">the (then) fastest platform to gain 100 million users</a>, a feat it pulled off in just two months. The company just teamed <a href="https://openai.com/index/mattels-iconic-brands/">up with Mattel</a>, which could result in a Barbie you can have a conversation with. The endless back-and-forth about intelligence seems abstract compared with the reality that hundreds of millions of people are using these systems to write emails, simulate tutors and even <a href="https://www.bbc.com/articles/c4nnje9rpjgo">fall in love</a> with their chat-partner avatars. The scholar Neil Postman’s idea about the rise of television — that we were “amusing ourselves to death” with the medium — could extend to A.I. You can’t become obsessed with something that isn’t amusing in the first place. No one ever fell in love with a calculator.</p><p>There’s a name for being fooled into thinking you’re dealing with an intelligent being when you’re not: the Eliza effect. The name comes from the first chatbot, built by the computer scientist Joseph Weizenbaum in 1966 and named for George Bernard Shaw’s character Eliza Doolittle. Mr. Weizenbaum thought of his program as a simple trick and was horrified when his secretary, testing it, asked him to leave the room because her conversation with the chatbot had become too intimate.</p><p>Critics of A.I. today seem to think that the whole world is under the spell of the Eliza effect, hundreds of millions of people deluded by gimmicks. But what if the effect is a sign of not delusion but a simple desire to keep chatting, to play with the limits of language?</p><p>When a Times of London journalist, James Mariott, <a href="https://x.com/j_amesmarriott/status/1930335137893359646">posted an A.I.-generated review</a> of Martin Amis’s novel “The Rachel Papers,” he set off a vehement debate about whether the passage was of magazine quality or shallow and repetitive. The argument wasn’t really about intelligence; it was about words. Underneath all the barbs and shouting — fueled by a broader public panic about a literacy crisis — I saw people reading and interpreting, passionately.</p><p>When I was a child, the panic was about video games, which were supposed to <a href="https://en.wikipedia.org/wiki/Family_Entertainment_Protection_Act">degrade us morally</a>, make us stupid and otherwise warp society permanently. None of that happened, even as video games became a globally popular form of entertainment.</p></div></div><div><div><p>Even the most serious face of A.I. — its ability to pass tests, solve difficult logic problems and math problems and hit benchmarks — can also be viewed as a form of entertainment: puzzles. Humans have always used cognitive challenges as a form of fun, and the history of A.I. is filled with these types of games, such as chess and Go.</p><p>One group of academics, led by the cognitive scientist Alison Gopnik, has characterized large language models as <a href="https://www.science.org/doi/abs/10.1126/science.adt9819">cultural technologies</a>, meaning that the bots contain an enormous amount of human knowledge, writing, images and other forms of cultural production. I tend to agree, but I think it’s crucial to understand that using such systems is also extremely entertaining. Whether you are <a href="https://www.reddit.com/r/ChatGPT/comments/140kd2o/i_extended_the_mona_lisa_painting_with_photoshop/">touching up the Mona Lisa</a>, reviewing novels or doing logic puzzles, you are engaging in the very human drive to play.</p><p>As I’ve watched people adopt these systems, what I’ve seen is mostly people playing with art and language. If you go through the history of these bots, you see poetry, fiction and all kinds of little genre experiments like this as a constantly recurring theme. Literary uses like this are deployed to <a href="https://openai.com/index/better-language-models/">advertise the bots</a> and their abilities, as well <a href="https://arxiv.org/abs/2005.14165">as quantitative metrics</a> to determine A.I.’s linguistic capabilities in formal data science papers. The effect isn’t limited to language, either. OpenAI also advertised one of its early models with an image produced by the prompt “<a href="https://x.com/OpenAI/status/1511714545529614338?lang=en">astronaut riding a horse</a>.” The natural response to this image is to think, “Cool!” A.I. is a culture machine.</p><p>That’s not to say I don’t share many of the worries of the critics. A.I.’s deployment in medicine, military applications, hiring algorithms and beyond is alarming. (Nobody should feel comfortable with the Defense Department’s intentions to <a href="https://www.washingtonpost.com/technology/2025/07/14/elon-musk-grok-defense-department/">use Elon Musk’s Grok chatbot</a>, so soon after it posted a deluge of <a href="https://www.nytimes.com/2025/07/12/technology/x-ai-grok-antisemitism.html">antisemitic comments</a>.) But I simply don’t think A.I. is driving the bus when it comes to these problems. Many of our systems are simply broken in the first place, and A.I. seems like a fix even when it isn’t.</p><p>We ought to think about A.I. as an entertainment system before anything else. Would you replace all of primary education with “Sesame Street”? Or decide government policy with SimCity? It’s not an insult to the beloved children’s program or computer game to say “no.” The lesson is simple: We might be taking A.I. too seriously.</p></div></div><div><div><p>Leif Weatherby (<a href="https://x.com/leifweatherby">@leifweatherby</a>) is an associate professor of German and the director of the Digital Theory Lab at New York University. He is the author of “<a href="https://www.upress.umn.edu/9781517919320/language-machines/">Language Machines</a>.”</p><p><em>The Times is committed to publishing </em><a href="https://www.nytimes.com/2019/01/31/opinion/letters/letters-to-editor-new-york-times-women.html"><em>a diversity of letters</em></a><em> to the editor. We’d like to hear what you think about this or any of our articles. Here are some </em><a href="https://help.nytimes.com/hc/en-us/articles/115014925288-How-to-submit-a-letter-to-the-editor"><em>tips</em></a><em>. And here’s our email: </em><a href="mailto:letters@nytimes.com"><em>letters@nytimes.com</em></a><em>.</em></p><p><em>Follow the New York Times Opinion section on </em><a href="https://www.facebook.com/nytopinion"><em>Facebook</em></a><em>, </em><a href="https://www.instagram.com/nytopinion/"><em>Instagram</em></a><em>, </em><a href="https://www.tiktok.com/@nytopinion"><em>TikTok</em></a><em>, </em><a href="https://bsky.app/profile/nytopinion.nytimes.com"><em>Bluesky</em></a>, <a href="https://www.whatsapp.com/channel/0029VaN8tdZ5vKAGNwXaED0M"><em>WhatsApp</em></a><em> and </em><a href="https://www.threads.net/@nytopinion"><em>Threads</em></a><em>.</em></p></div></div></section><div><div>A version of this article appears in print on <span> </span>, Section A, Page 18 of the New York edition with the headline: Stop Worrying and Have Fun With A.I.</div></div></article></pre>
    </div>
</body>
</html>